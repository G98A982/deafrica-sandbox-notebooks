{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://radiant-assets.s3-us-west-2.amazonaws.com/PrimaryRadiantMLHubLogo.png' alt='Radiant MLHub Logo' width='300'/>\n",
    "\n",
    "How to use the Radiant MLHub API\n",
    "=====\n",
    "\n",
    "The Radiant MLHub API gives access to open Earth imagery training data for machine learning applications. You can learn more about the repository at the [Radiant MLHub site](https://mlhub.earth) and about the organization behind it at the [Radiant Earth Foundation site](https://radiant.earth).\n",
    "\n",
    "This Jupyter notebook, which you may copy and adapt for any use, shows basic examples of how to use the API. Full documentation for the API is available at [docs.mlhub.earth](docs.mlhub.earth).\n",
    "\n",
    "We'll show you how to set up your authorization, see the list of available collections and datasets, and retrieve the items (the data contained within them) from those collections. \n",
    "\n",
    "Each item in our collection is explained in json format compliant with [STAC](https://stacspec.org/) [label extension](https://github.com/radiantearth/stac-spec/tree/master/extensions/label) definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authentication\n",
    "-----\n",
    "\n",
    "Access to the Radiant MLHub API requires an access token. To get your access token, go to [dashboard.mlhub.earth](https://dashboard.mlhub.earth). If you have not used Radiant MLHub before, you will need to sign up and create a new account. Otherwise, sign in. Under **Usage**, you'll see your access token, which you will need. *Do not share* your access token with others: your usage may be limited and sharing your access token is a security risk.\n",
    "\n",
    "Copy the access token, and paste it in the box bellow. This header block will work for all API calls.\n",
    "\n",
    "Click **Run** or press `SHIFT` + `ENTER` before moving on to run this first piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only the requests module is required to access the API\n",
    "import requests\n",
    "\n",
    "# copy your access token from dashboard.mlhub.earth and paste it in the following\n",
    "ACCESS_TOKEN = ''\n",
    "# these headers will be used in each request\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {ACCESS_TOKEN}',\n",
    "    'Accept':'application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for data collections\n",
    "-----\n",
    "\n",
    "To see what training data is available, you will want to see the collections available through the API.\n",
    "\n",
    "A collection represents the top-most data level. Typically this means the data comes from the same source for the same geography. It might include different years or sub-geographies.\n",
    "\n",
    "To find data with specific parameters, see the [API documentation](http://docs.mlhub.earth/?python#the-feature-collections-in-the-dataset).\n",
    "\n",
    "To see the list, simply run the following cell. The returned list shows the collection id values, collection license, and data source citation (if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all collections\n",
    "r = requests.get('https://api.radiant.earth/mlhub/v1/collections', headers=headers)\n",
    "h = r.json()\n",
    "collections = h['collections']\n",
    "\n",
    "# print the list of collections \n",
    "for c in collections:\n",
    "    print(f'ID:       {c[\"id\"]}\\nLicense:  {c.get(\"license\", \"N/A\")}\\nCitation: {c.get(\"sci:citation\", \"N/A\")}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve properties of a collection\n",
    "----\n",
    "\n",
    "Once you have found the collection that you want to access, you can get its properties from the API.\n",
    "\n",
    "You can  limit what data you get in the response using the optional parameters:\n",
    "* **Limit** limits how many items will be returned, with a minimum of 1 and maximum of 10000.\n",
    "* **Bounding box** limits the returned items to a specific geographic area. \n",
    "* **Date time** limits the returned items to those that fall within a specific time-frame.\n",
    "\n",
    "See the [get features](http://docs.mlhub.earth/#getfeatures) API documentation for more information.\n",
    "\n",
    "Paste the collection id below for `collectionId`, and enter any desired parameters, then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# paste the id of the collection you are interested in here:\n",
    "collectionId = 'ref_african_crops_kenya_02_labels'\n",
    "# use these optional parameters to control what items are returned. maximum limit is 10000\n",
    "limit = 10000\n",
    "bounding_box = []\n",
    "date_time = []\n",
    "\n",
    "# retrieves the items and their metadata in the collection\n",
    "r = requests.get(f'https://api.radiant.earth/mlhub/v1/collections/{collectionId}/items', params={'limit':limit, 'bbox':bounding_box,'datetime':date_time},headers=headers)\n",
    "collection = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 3 assets which match this criteria: `labels`, `documentation`, and `property descriptions`.\n",
    "\n",
    "Downloading Assets\n",
    "---\n",
    "We'll need to set up some functions to download assets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_download_url(item, asset_key, headers):\n",
    "    asset = item.get('assets', {}).get(asset_key, None)\n",
    "    if asset is None:\n",
    "        print(f'Asset \"{asset_key}\" does not exist in this item')\n",
    "        return None\n",
    "    r = requests.get(asset.get('href'), headers=headers, allow_redirects=False)\n",
    "    return r.headers.get('Location')\n",
    "\n",
    "def download_file(url):\n",
    "    filename = urlparse(url).path.split('/')[-1]\n",
    "    r = requests.get(url)\n",
    "    f = open(filename, 'wb')\n",
    "    for chunk in r.iter_content(chunk_size=512 * 1024): \n",
    "        if chunk:\n",
    "            f.write(chunk)\n",
    "    f.close()\n",
    "    print(f'Downloaded {filename}')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_item = None\n",
    "assets = None\n",
    "for feature in collection.get('features', []):\n",
    "    selected_item = feature\n",
    "    assets = list(feature.get('assets').keys())\n",
    "    download_file(get_download_url(selected_item, 'labels', headers))\n",
    "\n",
    "download_file(get_download_url(selected_item, 'documentation', headers))\n",
    "download_file(get_download_url(selected_item, 'property_descriptions', headers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangle\n",
    "\n",
    "Merge all the geojsons for each country, then join into one regional file from all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"data/training_validation/radiant_earth/tanzania/\"\n",
    "file = os.listdir(folder)\n",
    "path = [os.path.join(folder, i) for i in file if \".geojson\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(pd.concat([gpd.read_file(i) for i in path], \n",
    "                        ignore_index=True), crs=gpd.read_file(path[0]).crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(f'{folder}ref_african_crops_tanzania.geojson', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge into one regional file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'data/training_validation/radiant_earth/'\n",
    "\n",
    "tan = gpd.read_file(f'{folder}ref_african_crops_tanzania.geojson')[['Estimated Harvest Date', 'id', 'geometry']]\n",
    "ken = gpd.read_file(f'{folder}ref_african_crops_kenya.geojson')[['Estimated Harvest Date', 'id', 'geometry']]\n",
    "uga = gpd.read_file(f'{folder}ref_african_crops_uganda.geojson')[['Estimated Harvest Date', 'id', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_ref = pd.concat([tan,ken,uga])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_ref.to_file(f'{folder}ref_eastafrican_crops.geojson', driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
