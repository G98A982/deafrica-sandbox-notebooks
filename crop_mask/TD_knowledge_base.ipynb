{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting knowledge base of training data\n",
    "\n",
    "We want to see if the `feature layers` we are using for the ML crop/non-crop mask are actually useful for distinguishing the two classes. We can understand the knowledge base built by the training data in a couple of ways:\n",
    "\n",
    "1. Plotting class-specific `box plots` for each feature layer.\n",
    "\n",
    "2. Plot the class-specific `principal components` of each class e.g. PC1 vs PC2 as a scatter-plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install richdem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from feature_layer_extractions import phenology_features, two_epochs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_classificationtools import collect_training_data \n",
    "from deafrica_bandindices import calculate_indices\n",
    "from deafrica_temporal_statistics import xr_phenology, temporal_statistics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncpus= 14 #int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "\n",
    "# path = 'data/training_validation/training_polys_test_RE.shp' \n",
    "path = 'data/training_validation/GFSAD2015/cropland_prelim_validation_GFSAD_polys.shp' \n",
    "field = 'class'\n",
    "products =  ['s2_l2a'] #['ga_ls8c_gm_2_annual']\n",
    "time = ('2019-01', '2019-05')\n",
    "\n",
    "custom_func = two_epochs\n",
    "reduce_func = None #'mean'\n",
    "calc_indices = None#['NDVI'] \n",
    "drop = False\n",
    "zonal_stats = 'median' \n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open shapefile and ensure its in WGS84 coordinates\n",
    "input_data = gpd.read_file(path).to_crs('epsg:4326')\n",
    "\n",
    "#check the shapefile by plotting it\n",
    "# map_shapefile(input_data, attribute=field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip to AEZ if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aez = 'data/AEZs/Northern.shp'\n",
    "aez = gpd.read_file(aez).to_crs('epsg:4326')\n",
    "input_data = gpd.overlay(input_data, aez, how='intersection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a datacube query object\n",
    "query = {\n",
    "    'time': time,\n",
    "    'measurements': [\n",
    "                     'blue',\n",
    "                     'green',\n",
    "                     'red',\n",
    "                     'nir_1',\n",
    "                    ],\n",
    "    'resolution': (-20, 20),\n",
    "    'group_by' :'solar_day',\n",
    "    'output_crs':'epsg:6933'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "column_names, model_input = collect_training_data(ncpus=ncpus,\n",
    "                                    gdf=input_data,\n",
    "                                    products=products,\n",
    "                                    dc_query=query,\n",
    "                                    custom_func=custom_func,\n",
    "                                    field=field,\n",
    "                                    calc_indices=calc_indices,\n",
    "                                    reduce_func=reduce_func,\n",
    "                                    drop=drop,\n",
    "                                    zonal_stats=zonal_stats)\n",
    "\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seperate crop and non-crop classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = model_input[model_input[:,0]==1]\n",
    "non_crop = model_input[model_input[:,0]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert into pandas dataframes for ease**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_df = pd.DataFrame(crop).rename(columns={i:column_names[i] for i in range(0,len(column_names))}).drop('class', axis=1)\n",
    "non_crop_df = pd.DataFrame(non_crop).rename(columns={i:column_names[i] for i in range(0,len(column_names))}).drop('class', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalise values**\n",
    "\n",
    "So they can all plot on the same graph. Normalize by min/max values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_df = (crop_df-crop_df.min())/(crop_df.max()-crop_df.min())\n",
    "non_crop_df = (non_crop_df-non_crop_df.min())/(non_crop_df.max()-non_crop_df.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature layer boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(25,8))\n",
    "\n",
    "crop_col ='green'\n",
    "non_crop_col ='saddlebrown'\n",
    "\n",
    "pp = ax.boxplot(crop_df.values,\n",
    "                patch_artist=True,\n",
    "                showfliers=False, \n",
    "                positions=np.arange(crop_df.values.shape[1])-.2, widths=0.4,\n",
    "                boxprops=dict(facecolor=crop_col, color=crop_col),\n",
    "                capprops=dict(color=crop_col),\n",
    "                whiskerprops=dict(color=crop_col),\n",
    "                flierprops=dict(color=crop_col, markeredgecolor=crop_col),\n",
    "                medianprops=dict(color='black'))   \n",
    "\n",
    "fp = ax.boxplot(non_crop_df.values,\n",
    "                patch_artist=True, \n",
    "                showfliers=False,\n",
    "                positions=np.arange(crop_df.values.shape[1])+.2, widths=0.4,\n",
    "                boxprops=dict(facecolor=non_crop_col, color=non_crop_col),\n",
    "                capprops=dict(color=non_crop_col),\n",
    "                whiskerprops=dict(color=non_crop_col),\n",
    "                flierprops=dict(color=non_crop_col, markeredgecolor=non_crop_col),\n",
    "                medianprops=dict(color='black'))   \n",
    "\n",
    "ax.set_xticks(np.arange(len(column_names[1:])))\n",
    "ax.set_xticklabels(column_names[1:])\n",
    "ax.set_xlim(-0.5,len(column_names[1:])-.5)\n",
    "ax.set_ylabel(\"values\", fontsize=14)\n",
    "ax.set_xlabel(\"Bands\", fontsize=14)\n",
    "ax.set_title(\"Training Data Knowledge-Base\", fontsize=14)\n",
    "ax.legend([pp[\"boxes\"][0], fp[\"boxes\"][0]], ['Crop', 'Non-Crop'], loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Need to standardize the values before conducting the PCA.  Couple of ways to do this, using `StandardScaler` for now i.e. `x-u/s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all the model TD into a pandas dataframe\n",
    "all_data = pd.DataFrame(model_input).rename(columns={i:column_names[i] for i in range(0,len(column_names))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out the features\n",
    "x = all_data.loc[:, column_names[1:]].values\n",
    "\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conduct the PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_fit = pca.fit_transform(x)\n",
    "\n",
    "#add back to df\n",
    "pca_df = pd.DataFrame(data = pca_fit,\n",
    "                      columns = ['PC1', 'PC2'])\n",
    "\n",
    "# concat with classes\n",
    "df = pd.concat([pca_df, all_data[['class']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = pca.explained_variance_ratio_\n",
    "print(\"Variance explained by two principal components = \" + str(round((a+b)*100, 2))+\" %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the principal components\n",
    "\n",
    "Only plot a random subsample of the dataset as its huge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_points = 50000\n",
    "\n",
    "sns.lmplot(x=\"PC1\", y=\"PC2\",\n",
    "           data=df,#.sample(n=num_of_points),\n",
    "           fit_reg=False,\n",
    "           hue='class',\n",
    "           legend=False,\n",
    "           size=8,\n",
    "#            scatter_kws={'alpha':0.4},\n",
    "           markers=[\"^\", \".\"],\n",
    "           palette=['green', 'saddlebrown'])\n",
    "\n",
    "plt.legend(('crop', 'non-crop'), loc='lower right')\n",
    "plt.title('Training Data: Two Principal Components', fontsize=14)\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
