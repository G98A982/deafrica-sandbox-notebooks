{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampling of xarray DataArrays\n",
    "\n",
    "Testing a workflow for conducting random sampling on post-classification dataarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "from datacube.utils.cog import write_cog\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_plotting import map_shapefile\n",
    "from deafrica_spatialtools import xr_rasterize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 2000\n",
    "n_class= 2\n",
    "area_name='indianOcean'\n",
    "results = 'results/NDWI_composite/'+area_name+ '/'\n",
    "pred_tif = 'results/NDWI_composite/'+area_name+ '/'+ area_name+ '_NDWI_mosaic.tif'\n",
    "mask_shp = 'data/AEZs/AEZs_ExcludeLargeWB_IndianOcean.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and mask mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.open_rasterio(pred_tif).squeeze()\n",
    "    \n",
    "#load shapefile\n",
    "gdf = gpd.read_file(mask_shp)\n",
    "gdf = gdf.to_crs({'init': 'epsg:6933'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rasterize shapeile\n",
    "mask = xr_rasterize(gdf=gdf,\n",
    "                     da=da)\n",
    "\n",
    "da = da.where(mask)\n",
    "da = da.where(da!=0)\n",
    "da = da.to_dataset(name='ndwi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da.ndwi.isel(x=range(10000,20000)).isel(y=range(10000,20000)).plot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check NDWI distribution and determine thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(f'ndwi_{area_name}.csv'):\n",
    "if not os.path.exists(f'{results}ndwi_{area_name}.csv'):\n",
    "    histy, histx, tmp = da.ndwi.plot.hist(bins=100, cumulative=True, density=True);\n",
    "    np.savetxt(f'{results}ndwi_{area_name}.csv', np.vstack((histx[1:], histy)).transpose(),fmt='%.3f', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use wofs\n",
    "x, y = np.loadtxt(f'{results}wofs_{area_name}.csv', delimiter=',', unpack=True)\n",
    "ephem = 0.1\n",
    "perm = 0.9\n",
    "perc = np.interp([ephem, perm], x, y)\n",
    "print('percentile for ephemeral and permanent water', perc)\n",
    "histx, histy = np.loadtxt(f'{results}ndwi_{area_name}.csv', delimiter=',', unpack=True)\n",
    "thresh = np.interp(perc, histy, histx)\n",
    "print('Thresholds', thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify into dry, ephemeral and permanent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = thresh[0], thresh[1]\n",
    "\n",
    "label = np.zeros_like(da.ndwi.values, dtype=np.uint8)\n",
    "label += (da.ndwi.values >= high).astype(np.uint8)*3\n",
    "label += ((da.ndwi.values >= low) & (da.ndwi.values<high)).astype(np.uint8)*2\n",
    "label +=(da.ndwi.values < low).astype(np.uint8)*1\n",
    "da['label'] = ('y','x'), label\n",
    "da['label'].attrs = da.ndwi.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cog(da.label, f'{results}{area_name}_label.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sizes =[]\n",
    "for class_id in np.arange(1, n_class+1):\n",
    "    class_sizes.append((da.label==class_id).sum().values)\n",
    "\n",
    "class_sizes = np.array(class_sizes)\n",
    "print(class_sizes)\n",
    "print(class_sizes/class_sizes.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_class = np.ceil(n_sample*1./ n_class).astype(int)\n",
    "print(n_sample_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_picked = {}\n",
    "for class_id in np.arange(1, n_class+1):\n",
    "    if class_sizes[class_id-1]> 1e9:\n",
    "        # slightly over sample\n",
    "        n_sample_over = np.ceil(n_sample_class*len(da.x)*len(da.y)/class_sizes[class_id-1]).astype(int)\n",
    "        random_x = np.random.choice(np.arange(len(da.x)), n_sample_over, replace=False)\n",
    "        random_y = np.random.choice(np.arange(len(da.y)), n_sample_over, replace=False)\n",
    "        match = dataset.label.values[random_y, random_x] == class_id\n",
    "        random_y, random_x = random_y[match], random_x[match]\n",
    "        if len(random_y) < n_sample_class:\n",
    "            print(\"Not enough points are picked, try increase the number of random points\")\n",
    "            break\n",
    "        else:\n",
    "            pick = np.random.choice(np.arange(len(random_y)), n_sample_class, replace=False)\n",
    "            y, x = random_y[pick], random_x[pick]\n",
    "    else:\n",
    "        index = np.argwhere(da.label.values.flatten() == class_id).squeeze()\n",
    "        picked = np.random.choice(index, n_sample_class, replace=False)\n",
    "        # convert back to x, y \n",
    "        y, x  = np.unravel_index(picked, da.label.values.shape)\n",
    "    label_picked[class_id] = (y, x)\n",
    "    np.savetxt(f'{results}{area_name}_class_{class_id}.csv', \n",
    "               np.vstack((da.y[y].values, \n",
    "                          da.x[x].values)).transpose(),fmt='%d', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_id in np.arange(1, n_class+1):\n",
    "    y, x = label_picked[class_id]\n",
    "    df = pd.DataFrame({'y': da.y[y].values, 'x':da.x[x].values})\n",
    "    df['class']=class_id\n",
    "    if class_id ==1: \n",
    "        dfs = df\n",
    "    else: \n",
    "        dfs = dfs.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "        dfs,\n",
    "        crs=da.label.crs,\n",
    "        geometry=gpd.points_from_xy(dfs.x, dfs.y)).reset_index()\n",
    "\n",
    "gdf = gdf.drop(['x', 'y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot(column='class', figsize=(15,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(results+'indianOcean_samples.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #open\n",
    "# da = xr.open_rasterio(pred_tif).squeeze()\n",
    "\n",
    "# #reclassify\n",
    "# da = xr.where(da >= 0, 3, da)\n",
    "# da = xr.where((da >=-0.1) & (da < 0), 2, da)\n",
    "# da = xr.where(da <-0.1, 1, da)\n",
    "\n",
    "# #minimize data size by convertng to int8\n",
    "# da = da.fillna(0).astype(np.int8).assign_coords({'x': da.x.astype(np.float32).values,\n",
    "#                                                   'y':da.y.astype(np.float32).values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_sampling(da,\n",
    "#                     n,\n",
    "#                     sampling='stratified_random',\n",
    "#                     manual_class_ratios=None,\n",
    "#                     out_fname=None\n",
    "#                    ):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Creates randomly sampled points for post-classification\n",
    "#     accuracy assessment.\n",
    "    \n",
    "#     Params:\n",
    "#     -------\n",
    "#     da: xarray.DataArray\n",
    "#         A classified 2-dimensional xarray.DataArray\n",
    "#     n: int\n",
    "#         Total number of points to sample. Ignored if providing\n",
    "#         a dictionary of {class:numofpoints} to 'manual_class_ratios'\n",
    "#     sampling: str\n",
    "#         'stratified_random' = Create points that are randomly \n",
    "#         distributed within each class, where each class has a\n",
    "#         number of points proportional to its relative area. \n",
    "#         'equal_stratified_random' = Create points that are randomly\n",
    "#         distributed within each class, where each class has the\n",
    "#         same number of points.\n",
    "#         'random' = Create points that are randomly distributed\n",
    "#         throughout the image.\n",
    "#         'manual' = user definined, each class is allocated a \n",
    "#         specified number of points, supply a manual_class_ratio \n",
    "#         dictionary mapping number of points to each class\n",
    "#     manual_class_ratios: dict\n",
    "#         If setting sampling to 'manual', the provide a dictionary\n",
    "#         of type {'class': numofpoints} mapping the number of points\n",
    "#         to generate for each class.\n",
    "#     out_fname: str\n",
    "#         If providing a filepath name, e.g 'sample_points.shp', the\n",
    "#         function will export a shapefile/geojson of the sampling\n",
    "#         points to file.\n",
    "    \n",
    "#     Output\n",
    "#     ------\n",
    "#     GeoPandas.Dataframe\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     if sampling not in ['stratified_random', 'equal_stratified_random', 'random', 'manual']:\n",
    "#         raise ValueError(\"Sampling strategy must be one of 'stratified_random', \"+\n",
    "#                              \"'equal_stratified_random', 'random', or 'manual'\") \n",
    "#     print('here')\n",
    "#     #open the dataset as a pandas dataframe\n",
    "# #     df = da.squeeze()\n",
    "#     df = da.to_dataframe(name='class')#.astype('category')\n",
    "#     print('made the dataframe')\n",
    "#     #list to store points\n",
    "#     samples = []\n",
    "    \n",
    "#     if sampling == 'stratified_random':\n",
    "#         #determine class ratios in image\n",
    "#         class_ratio = pd.DataFrame({'proportion': df['class'].value_counts(normalize=True),\n",
    "#                             'class':df['class'].unique()\n",
    "#                                  })\n",
    "        \n",
    "#         for _class in class_ratio['class']:\n",
    "#             #use relative proportions of classes to sample df\n",
    "#             no_of_points = n * class_ratio[class_ratio['class']==_class]['proportion'].values[0]\n",
    "#             #random sample each class\n",
    "#             print('Class '+ str(_class)+ ': sampling at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#             sample_loc = df[df['class'] == _class].sample(n=int(round(no_of_points)))\n",
    "#             samples.append(sample_loc)\n",
    "\n",
    "#     if sampling == 'equal_stratified_random':\n",
    "#         classes = df['class'].unique()\n",
    "        \n",
    "#         for _class in classes:\n",
    "            \n",
    "#             no_of_points = n / len(classes)\n",
    "#             #random sample each classes\n",
    "#             try:\n",
    "#                 sample_loc = df[df['class'] == _class].sample(n=int(round(no_of_points)))\n",
    "#                 print('Class '+ str(_class)+ ': sampling at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#                 samples.append(sample_loc)\n",
    "            \n",
    "#             except ValueError:\n",
    "#                         print('Requested more sample points than population of pixels for class '+ str(_class)+', skipping')\n",
    "#                         pass\n",
    "    \n",
    "#     if sampling == 'random':\n",
    "#         no_of_points = n\n",
    "#         #random sample entire df\n",
    "#         print('Randomly sampling dataAraay at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#         sample_loc = df.sample(n=int(round(no_of_points)))\n",
    "#         samples.append(sample_loc)\n",
    "    \n",
    "#     if sampling == 'manual':\n",
    "#         if isinstance(manual_class_ratios, dict):\n",
    "#             for _class in list(manual_class_ratios.keys()):\n",
    "#                 no_of_points = manual_class_ratios.get(str(_class))\n",
    "                \n",
    "#                 try:\n",
    "#                     print('sampling '+ _class)\n",
    "#                     sample_loc = df[df['class'] == int(_class)].sample(n=int(round(no_of_points)))\n",
    "#                     print('Class '+ str(_class)+ ': sampled at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#                     samples.append(sample_loc)\n",
    "\n",
    "#                 except ValueError:\n",
    "#                     print('Requested more sample points than population of pixels for class '+ str(_class)+', skipping')\n",
    "#                     pass\n",
    "            \n",
    "#         else:\n",
    "#             raise ValueError(\"Must supply a dictionary mapping {'class': numofpoints} if sampling\" +\n",
    "#                              \" is set to 'manual'\")\n",
    "    \n",
    "#     #join back into single datafame\n",
    "#     all_samples = pd.concat([samples[i] for i in range(0,len(samples))])\n",
    "        \n",
    "#     #get pd.mulitindex coords as list \n",
    "#     y = [i[0] for i in list(all_samples.index)]\n",
    "#     x = [i[1] for i in list(all_samples.index)]\n",
    "\n",
    "#     #create geopandas dataframe\n",
    "#     gdf = gpd.GeoDataFrame(\n",
    "#         all_samples,\n",
    "#         crs=da.crs,\n",
    "#         geometry=gpd.points_from_xy(x,y)).reset_index()\n",
    "\n",
    "#     gdf = gdf.drop(['x', 'y'],axis=1)\n",
    "    \n",
    "#     if out_fname is not None:\n",
    "#         gdf.to_file(out_fname)\n",
    "    \n",
    "#     return gdf\n",
    "\n",
    "\n",
    "# %%time\n",
    "# gdf = random_sampling(da=da,\n",
    "#                     n=total_points,\n",
    "#                     sampling='manual',\n",
    "#                     manual_class_ratios={'1':167, '2':167, '3':167},\n",
    "#                     out_fname='results/test_western.shp'\n",
    "#                        )\n",
    "\n",
    "# gdf.plot(column='class', figsize=(10,10),legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
