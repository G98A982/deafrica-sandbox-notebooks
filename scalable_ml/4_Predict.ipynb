{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install richdem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "from odc.algo import xr_geomedian, int_geomedian\n",
    "import xarray as xr\n",
    "import subprocess as sp\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import assign_crs\n",
    "import matplotlib.pyplot as plt\n",
    "# import richdem as rd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_datahandling import load_ard\n",
    "from deafrica_classificationtools import predict_xr ,predict_proba_xr\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "from deafrica_plotting import map_shapefile, rgb\n",
    "from deafrica_bandindices import calculate_indices\n",
    "from deafrica_temporal_statistics import temporal_statistics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask cluster\n",
    "This will help keep our memory use down and conduct the analysis in parallel. If you'd like to view the dask dashboard, click on the hyperlink that prints below the cell. You can use the dashboard to monitor the progress of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis parameters\n",
    "\n",
    "* `ncpus`: Set this value to > 1 to parallelize the collection of training data. eg. npus=8. \n",
    "* `model`: Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically detect number of cpus, adjust to [-3:] if working on deafault Sandbox\n",
    "ncpus= int(float(sp.getoutput('env | grep CPU')[-4:]))\n",
    "\n",
    "model_path = 'results/ml_model.joblib'\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some test locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = {'1':(12.2636, 37.0244),\n",
    "             '2':(-0.2953, 38.4422),\n",
    "             '3':(11.5217, 37.4894),\n",
    "             '4':(-0.6294, 34.1357),\n",
    "             '5':(-0.3878, 37.4869),\n",
    "             '6':(-3.4866, 37.3650),\n",
    "             '7':(-0.7062, 36.5865)\n",
    "            }\n",
    "\n",
    "buffer = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction\n",
    "\n",
    "Extract data from the datacube exactly matching the feature layers we created during the extraction of training data in script `1_Extract_training_data.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xr_geomedian_tmad import xr_geomedian_tmad\n",
    "\n",
    "def two_seasons_gm_mads(ds):\n",
    "    \n",
    "    ds = ds / 10000\n",
    "    ds1 = ds.sel(time=slice('2019-01', '2019-06'))\n",
    "    ds2 = ds.sel(time=slice('2019-07', '2019-12')) \n",
    "    \n",
    "    def fun(ds, era):\n",
    "        \n",
    "        #band indices\n",
    "        bi = calculate_indices(ds,\n",
    "                               index=['EVI', 'LAI'],\n",
    "                               drop=True,\n",
    "                               normalise=False,\n",
    "                               collection='s2')\n",
    "        \n",
    "        bi_max = bi.max('time')\n",
    "        bi_min = bi.min('time')\n",
    "        bi_range = bi_max - bi_min\n",
    "        bi_range = bi_range.rename({'EVI':'EVI_range','LAI':'LAI_range'})\n",
    "        bi_max = bi_max.rename({'EVI':'EVI_max','LAI':'LAI_max'})\n",
    "        bi_min = bi_min.rename({'EVI':'EVI_min','LAI':'LAI_min'})\n",
    "        \n",
    "        #geomedian and tmads\n",
    "        gm_mads = xr_geomedian_tmad(ds)\n",
    "        gm_mads = calculate_indices(gm_mads,\n",
    "                               index=['EVI', 'LAI'],\n",
    "                               drop=False,\n",
    "                               normalise=False,\n",
    "                               collection='s2')\n",
    "        \n",
    "        gm_mads['edev'] = -np.log(gm_mads['edev'])\n",
    "        gm_mads['sdev'] = -np.log(gm_mads['sdev'])\n",
    "        gm_mads['bcdev'] = -np.log(gm_mads['bcdev'])\n",
    "        \n",
    "        out = xr.merge([gm_mads,bi_max,bi_min,bi_range],compat='override')\n",
    "        \n",
    "        for band in out.data_vars:\n",
    "            out = out.rename({band:band+era})\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    epoch1 = fun(ds1, era='_S1')\n",
    "    epoch2 = fun(ds2, era='_S2')\n",
    "    \n",
    "    result = xr.merge([epoch1,\n",
    "                       epoch2],\n",
    "                      compat='override')\n",
    "\n",
    "    return result.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our inputs to collect_training_data\n",
    "products =  ['s2_l2a']\n",
    "time = ('2019-01','2019-12')\n",
    "\n",
    "# Set up the inputs for the ODC query\n",
    "measurements =  ['red', 'green', 'nir', 'blue', 'swir_1', 'swir_2']\n",
    "resolution = (-20,20)\n",
    "output_crs='epsg:6933'\n",
    "dask_chunks={'x':1000,'y':1000,'time':-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "prediction_proba = []\n",
    "features = []\n",
    "# for index, row in gdf.iloc[list_of_tiles].iterrows():\n",
    "for key, latlon in locations.items():\n",
    "    print(\"Working on : \"+str(latlon))\n",
    "    \n",
    "    # generate a datacube query object\n",
    "    query = {\n",
    "        'x': (latlon[1]-buffer, latlon[1]+buffer),\n",
    "        'y': (latlon[0]+buffer, latlon[0]-buffer),\n",
    "        'time': time,\n",
    "        'measurements': measurements,\n",
    "        'resolution': resolution,\n",
    "        'output_crs': output_crs,\n",
    "        'group_by' : 'solar_day',\n",
    "    }\n",
    "\n",
    "    ds = load_ard(dc=dc,\n",
    "                  products=products,\n",
    "                  dask_chunks=dask_chunks,\n",
    "                  min_gooddata=0.15,\n",
    "                  **query)\n",
    "    \n",
    "    data = two_seasons_gm_mads(ds)\n",
    "    data = data.compute()\n",
    "    \n",
    "    #predict using the imported model\n",
    "    print('predicting...')\n",
    "    predicted = predict_xr(model, data, progress=True)\n",
    "    probability = predict_proba_xr(model, data.squeeze(), progress=True)\n",
    "    s\n",
    "    predictions.append(predicted)\n",
    "    prediction_proba.append(probability)\n",
    "    features.append(data)\n",
    "    \n",
    "    write_cog(predicted, 'results/classifications/Eastern_tile'+key+'_prediction.tif', overwrite=True)\n",
    "    write_cog(probability, 'results/classifications/Eastern_tile'+key+'_probability.tif', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 12))\n",
    "\n",
    "# Plot classified image\n",
    "predictions[idx].plot(ax=axes[0], \n",
    "               cmap='Greens', \n",
    "               add_labels=False, \n",
    "               add_colorbar=False)\n",
    "\n",
    "# Plot true colour image\n",
    "rgb(features[idx], bands=['red_S2','green_S2','blue_S2'],\n",
    "    ax=axes[1], percentile_stretch=(0.01, 0.99))\n",
    "\n",
    "probability[idx].plot(ax=axes[2], \n",
    "               cmap='plasma',\n",
    "               vmin=0,\n",
    "               vmax=100,\n",
    "               add_labels=False, \n",
    "               add_colorbar=True)\n",
    "\n",
    "# Remove axis on right plot\n",
    "axes[2].get_yaxis().set_visible(False)\n",
    "\n",
    "# Add plot titles\n",
    "axes[0].set_title('Classified Image')\n",
    "axes[1].set_title('True Colour Image')\n",
    "axes[2].set_title('Probabilities');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr = ds.red.isel(time=0)\n",
    "# template = xr.zeros_like(arr).to_dataset(name='edev')\n",
    "# template['sdev'] = xr.zeros_like(arr)\n",
    "# template['bcdev'] = xr.zeros_like(arr)\n",
    "# template = template.drop(['spatial_ref', 'time'])\n",
    "# template\n",
    "\n",
    "# stats = TernaryMAD(num_threads=1)\n",
    "# mad = ds.chunk(d).map_blocks(\n",
    "#             stats.compute(data=ds), template=template\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
