{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Supervised Machine Learning on the ODC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a placeholder for now.  It will contain a markdown only description of the scripts in this folder, including:\n",
    "- The guiding principles/philosophy of the scripts (ie setting expectations around what they can and cannot do)\n",
    "- The order to run the scripts in (including links to each script)\n",
    "- Any prerequisites required to understand the content in these notebooks (e.g. reading material, ODC/Sandbox/notebooks/python training material)\n",
    "- Information on any set-up that might be required to run these notebooks (e.g. creating folders for outputs? Setting up a S3 bucket for intermediate files?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TO DO\n",
    "- add a `debug=True|False` flag to `collect_training_data`, this will set ncpus=1 and remove HiddenPrints()\n",
    "- add WOfS loading option to `collect_training_data`?\n",
    "- add a check to the `collect_training_data` function that ensures the classes are integers, currently it runs the whole script before crashing at the last step\n",
    "- See if we can merge `predict_xr` with `predict_proba_xr` as its silly to have to load all the data twice... not sure how yet: maybe with two apply_ufuncs? Or, stack predict&proba together as arrays as per https://stackoverflow.com/questions/52094320/with-xarray-how-to-parallelize-1d-operations-on-a-multidimensional-dataset\n",
    "- rewrite `predict_xr` in light of `dask-ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xr(model, input_xr):\n",
    "    with joblib.parallel_backend('dask'):\n",
    "        \n",
    "        input_data = []\n",
    "\n",
    "        for var_name in input_xr.data_vars:\n",
    "            input_data.append(input_xr[var_name])\n",
    "\n",
    "        input_data_flattened = []\n",
    "        for data in args:\n",
    "            input_data_flattened.append(data.flatten())\n",
    "\n",
    "        # Flatten array\n",
    "        input_data_flattened = np.array(input_data_flattened).transpose()\n",
    "\n",
    "        # Mask out no-data in input (not all classifiers can cope with\n",
    "        # Inf or NaN values)\n",
    "        input_data_flattened = np.where(np.isfinite(input_data_flattened),\n",
    "                                        input_data_flattened, 0)\n",
    "\n",
    "        # Actually apply the classification\n",
    "        out_class = model.predict(input_data_flattened)\n",
    "\n",
    "        # Mask out NaN or Inf values in results\n",
    "        out_class = np.where(np.isfinite(out_class), out_class, 0)\n",
    "\n",
    "        # Reshape when writing out\n",
    "        out_class = out_class.reshape(args[0].shape)\n",
    "\n",
    "        # Set the stacked coordinate to match the input\n",
    "        output_xr = xr.DataArray(out_class, coords=input_xr.coords)\n",
    "        \n",
    "        return output_xr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
