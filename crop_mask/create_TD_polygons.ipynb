{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating random training data polygons\n",
    "\n",
    "Using Collect Earth Online (CEO), these randomly scattered polygons will be classified as crop, non-crop, or mixed and form the knowledge base for the machine learning classifier.\n",
    "\n",
    "Stratifying points using the GFSAD cropland extent map (30m resolution).\n",
    "\n",
    "    gdalwarp -tr 0.0005389891704717132 0.0005389891704717132  -r mode GFSAD_mosaic.tif GFSAD_mosaic_60m.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from datacube.utils.cog import write_cog\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datacube\n",
    "from affine import Affine\n",
    "\n",
    "import sys\n",
    "sys.path.append('../Scripts')\n",
    "from deafrica_spatialtools import xr_rasterize\n",
    "from deafrica_dask import create_local_dask_cluster\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asap_crop_mask = 'data/asap_mask_crop_v03.tif'\n",
    "crop_mask_shp = 'data/GFSAD/GFSAD_mosaic.tif'\n",
    "\n",
    "#AEZ to mask crop mask\n",
    "aez = 'data/AEZs/Eastern.shp'\n",
    "\n",
    "#location and file name to store shapefiles.\n",
    "results = 'data/training_validation/collect_earth/Eastern_TD'\n",
    "\n",
    "#total number of sample points\n",
    "n_sample = 2200\n",
    "\n",
    "#nuof classes to sample\n",
    "n_class= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open AEZ shapefile\n",
    "aez = gpd.read_file(aez)\n",
    "\n",
    "#find bouding box\n",
    "xmin,ymin,xmax,ymax = aez.bounds.values[0][0], aez.bounds.values[0][1], aez.bounds.values[0][2], aez.bounds.values[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open GFSAD cropmask at bounding coords of AEZ\n",
    "da = xr.open_rasterio(crop_mask_shp).squeeze().sel(x=slice(xmin,xmax),\n",
    "                                                  y=slice(ymax,ymin)).chunk({'x':3000, 'y':3000})\n",
    "attrs = da.attrs\n",
    "#recreate transform object\n",
    "transform = Affine(da.transform[0], 0.0, da.x.values[0], 0.0, da.transform[4], da.y.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask by AEZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aez_raster = xr_rasterize(aez, da,\n",
    "                         transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = da.where(aez_raster).astype(np.int8)\n",
    "da = da.to_dataset(name='GFSAD')\n",
    "da.attrs = attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = da.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_cog(crop_mask,\n",
    "#           'data/GFSAD/' + name + '_GFSAD.tif')\n",
    "\n",
    "# from datacube.helpers import write_geotiff\n",
    "# write_geotiff('data/GFSAD/' + name + '_GFSAD.tif',\n",
    "#                 crop_mask.to_dataset(name='Southern_GFSAD'),\n",
    "#              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random points\n",
    "\n",
    "2000 random points: 50% (1000) in 2, 25% (500) in 1, 25% (500) in 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_sizes =[]\n",
    "for class_id in np.arange(1, n_class+1):\n",
    "    print(class_id)\n",
    "    class_sizes.append((da.GFSAD==class_id).sum().values)\n",
    "\n",
    "class_sizes = np.array(class_sizes)\n",
    "print(class_sizes)\n",
    "print(class_sizes/class_sizes.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_class = np.ceil(n_sample*1./ n_class).astype(int)\n",
    "print(n_sample_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_picked = {}\n",
    "for class_id in np.arange(1, n_class+1):\n",
    "    if class_sizes[class_id-1]> 1e9:\n",
    "        # slightly over sample\n",
    "        n_sample_over = np.ceil(n_sample_class*len(da.x)*len(da.y)/class_sizes[class_id-1]).astype(int)\n",
    "        random_x = np.random.choice(np.arange(len(da.x)), n_sample_over, replace=False)\n",
    "        random_y = np.random.choice(np.arange(len(da.y)), n_sample_over, replace=False)\n",
    "        match = da.GFSAD.values[random_y, random_x] == class_id\n",
    "        random_y, random_x = random_y[match], random_x[match]\n",
    "        if len(random_y) < n_sample_class:\n",
    "            print(\"Not enough points are picked, try increase the number of random points\")\n",
    "            break\n",
    "        else:\n",
    "            pick = np.random.choice(np.arange(len(random_y)), n_sample_class, replace=False)\n",
    "            y, x = random_y[pick], random_x[pick]\n",
    "    else:\n",
    "        index = np.argwhere(da.GFSAD.values.flatten() == class_id).squeeze()\n",
    "        picked = np.random.choice(index, n_sample_class, replace=False)\n",
    "        # convert back to x, y \n",
    "        y, x  = np.unravel_index(picked, da.GFSAD.values.shape)\n",
    "    label_picked[class_id] = (y, x)\n",
    "    np.savetxt(f'{results}_class_{class_id}.csv', \n",
    "               np.vstack((da.y[y].values, \n",
    "                          da.x[x].values)).transpose(),fmt='%d', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_id in np.arange(1, n_class+1):\n",
    "    y, x = label_picked[class_id]\n",
    "    df = pd.DataFrame({'y': da.y[y].values, 'x':da.x[x].values})\n",
    "    df['class']=class_id\n",
    "    if class_id ==1: \n",
    "        dfs = df\n",
    "    else: \n",
    "        dfs = dfs.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_points = gpd.GeoDataFrame(\n",
    "            dfs,\n",
    "            crs=da.crs,\n",
    "            geometry=gpd.points_from_xy(dfs.x, dfs.y)).reset_index()\n",
    "\n",
    "gdf_points = gdf_points.drop(['x', 'y'],axis=1)\n",
    "\n",
    "#add a PLOTID field to satisfy CEO\n",
    "gdf_points['PLOTID'] = range(0,len(gdf_points))\n",
    "gdf_points['SAMPLEID'] = range(0,len(gdf_points))\n",
    "\n",
    "#shuffle rows so classes aren't sequential in CEO\n",
    "gdf_points = gdf_points.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_points.plot(figsize=(8,6), column='class', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert points to square polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_points = gpd.read_file(results + '_points.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set radius (in metres) around points\n",
    "radius = 30\n",
    "\n",
    "#convert to equal area to set polygon size in metres\n",
    "gdf_poly = gdf_points.to_crs('EPSG:6933')\n",
    "\n",
    "#create circle buffer around points, then find envelope\n",
    "gdf_poly['geometry'] = gdf_poly['geometry'].buffer(radius).envelope\n",
    "\n",
    "#Convert back to lat/lon\n",
    "gdf_poly = gdf_poly.to_crs('EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to file\n",
    "gdf_poly.to_file(results + '_polys.shp')\n",
    "\n",
    "gdf_points.to_file(results + '_points.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_sampling(da,\n",
    "#                     n,\n",
    "#                     sampling='stratified_random',\n",
    "#                     manual_class_ratios=None,\n",
    "#                     out_fname=None\n",
    "#                    ):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     Creates randomly sampled points for post-classification\n",
    "#     accuracy assessment.\n",
    "    \n",
    "#     Params:\n",
    "#     -------\n",
    "#     da: xarray.DataArray\n",
    "#         A classified 2-dimensional xarray.DataArray\n",
    "#     n: int\n",
    "#         Total number of points to sample. Ignored if providing\n",
    "#         a dictionary of {class:numofpoints} to 'manual_class_ratios'\n",
    "#     sampling: str\n",
    "#         'stratified_random' = Create points that are randomly \n",
    "#         distributed within each class, where each class has a\n",
    "#         number of points proportional to its relative area. \n",
    "#         'equal_stratified_random' = Create points that are randomly\n",
    "#         distributed within each class, where each class has the\n",
    "#         same number of points.\n",
    "#         'random' = Create points that are randomly distributed\n",
    "#         throughout the image.\n",
    "#         'manual' = user definined, each class is allocated a \n",
    "#         specified number of points, supply a manual_class_ratio \n",
    "#         dictionary mapping number of points to each class\n",
    "#     manual_class_ratios: dict\n",
    "#         If setting sampling to 'manual', the provide a dictionary\n",
    "#         of type {'class': numofpoints} mapping the number of points\n",
    "#         to generate for each class.\n",
    "#     out_fname: str\n",
    "#         If providing a filepath name, e.g 'sample_points.shp', the\n",
    "#         function will export a shapefile/geojson of the sampling\n",
    "#         points to file.\n",
    "    \n",
    "#     Output\n",
    "#     ------\n",
    "#     GeoPandas.Dataframe\n",
    "    \n",
    "#     \"\"\"\n",
    "    \n",
    "#     if sampling not in ['stratified_random', 'equal_stratified_random', 'random', 'manual']:\n",
    "#         raise ValueError(\"Sampling strategy must be one of 'stratified_random', \"+\n",
    "#                              \"'equal_stratified_random', 'random', or 'manual'\") \n",
    "    \n",
    "#     #open the dataset as a pandas dataframe\n",
    "#     da = da.squeeze()\n",
    "#     df = da.to_dataframe(name='class')\n",
    "    \n",
    "#     #list to store points\n",
    "#     samples = []\n",
    "    \n",
    "#     if sampling == 'stratified_random':\n",
    "#         #determine class ratios in image\n",
    "#         class_ratio = pd.DataFrame({'proportion': df['class'].value_counts(normalize=True),\n",
    "#                             'class':np.unique(df['class'])\n",
    "#                                  })\n",
    "        \n",
    "#         for _class in class_ratio['class']:\n",
    "#             #use relative proportions of classes to sample df\n",
    "#             no_of_points = n * class_ratio[class_ratio['class']==_class]['proportion'].values[0]\n",
    "#             #random sample each class\n",
    "#             print('Class '+ str(_class)+ ': sampling at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#             sample_loc = df[df['class'] == _class].sample(n=int(round(no_of_points)))\n",
    "#             samples.append(sample_loc)\n",
    "\n",
    "#     if sampling == 'equal_stratified_random':\n",
    "#         classes = np.unique(df['class'])\n",
    "        \n",
    "#         for _class in classes:\n",
    "#             #use relative proportions of classes to sample df\n",
    "#             no_of_points = n / len(classes)\n",
    "#             #random sample each classes\n",
    "#             try:\n",
    "#                 sample_loc = df[df['class'] == _class].sample(n=int(round(no_of_points)))\n",
    "#                 print('Class '+ str(_class)+ ': sampling at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#                 samples.append(sample_loc)\n",
    "            \n",
    "#             except ValueError:\n",
    "#                         print('Requested more sample points than population of pixels for class '+ str(_class)+', skipping')\n",
    "#                         pass\n",
    "    \n",
    "#     if sampling == 'random':\n",
    "#         no_of_points = n\n",
    "#         #random sample entire df\n",
    "#         print('Randomly sampling dataAraay at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#         sample_loc = df.dropna().sample(n=int(round(no_of_points)))\n",
    "#         samples.append(sample_loc)\n",
    "    \n",
    "#     if sampling == 'manual':\n",
    "#         if isinstance(manual_class_ratios, dict):\n",
    "#             #check classes in dict match classes in data\n",
    "#             classes = np.unique(df['class'])\n",
    "#             dict_classes = list(manual_class_ratios.keys())\n",
    "            \n",
    "#             if set(dict_classes).issubset([str(i) for i in classes]):\n",
    "#                 #mask for just those classes in the provided dictionary\n",
    "#                 mask = np.isin(classes,\n",
    "#                                np.array(dict_classes).astype(type(classes[0])))\n",
    "#                 classes = classes[mask]               \n",
    "#                 #run sampling\n",
    "#                 for _class in classes:\n",
    "#                     no_of_points = manual_class_ratios.get(str(_class))\n",
    "#                     #random sample each class\n",
    "#                     try:\n",
    "#                         sample_loc = df[df['class'] == _class].sample(n=int(round(no_of_points)))\n",
    "#                         print('Class '+ str(_class)+ ': sampled at '+ str(round(no_of_points)) + ' coordinates')\n",
    "#                         samples.append(sample_loc)\n",
    "                        \n",
    "#                     except ValueError:\n",
    "#                         print('Requested more sample points than population of pixels for class '+ str(_class)+', skipping')\n",
    "#                         pass\n",
    "\n",
    "#             else:\n",
    "#                 raise ValueError(\"Some or all of the classes in 'manual_class_ratio' dictionary do not\" +\n",
    "#                                  \" match the classes in the supplied dataArray. \"+\n",
    "#                                 \"DataArray classes: \"+str(classes)+\", Supplied dict classes: \"+\n",
    "#                                  str(list(manual_class_ratios.keys())))\n",
    "            \n",
    "#         else:\n",
    "#             raise ValueError(\"Must supply a dictionary mapping {'class': numofpoints} if sampling\" +\n",
    "#                              \" is set to 'manual'\")\n",
    "    \n",
    "#     #join back into single datafame\n",
    "#     all_samples = pd.concat([samples[i] for i in range(0,len(samples))])\n",
    "        \n",
    "#     #get pd.mulitindex coords as list \n",
    "#     y = [i[0] for i in list(all_samples.index)]\n",
    "#     x = [i[1] for i in list(all_samples.index)]\n",
    "\n",
    "#     #create geopandas dataframe\n",
    "#     gdf = gpd.GeoDataFrame(\n",
    "#         all_samples,\n",
    "#         crs=da.crs,\n",
    "#         geometry=gpd.points_from_xy(x,y)).reset_index()\n",
    "\n",
    "#     gdf = gdf.drop(['x', 'y'],axis=1)\n",
    "    \n",
    "#     if out_fname is not None:\n",
    "#         gdf.to_file(out_fname)\n",
    "    \n",
    "#     return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
