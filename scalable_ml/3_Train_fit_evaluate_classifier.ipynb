{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, fit, and evaluate classifier <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "\n",
    "TODO:\n",
    "- if/when datasets become large, consider implementing `dask_ml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "blahblahblah\n",
    "\n",
    "\n",
    "_Table 1: Some of the pros and cons of different classifiers available through scikit-learn_\n",
    "<img align=\"center\" src=\"classifier_pro_cons.PNG\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "1. Split the training data into a training set and a test set\n",
    "2. Optionally standardise the datasets\n",
    "2. Train a classifier\n",
    "3. Evaluate the classifier using a number of metrics\n",
    "4. Optimise the model hyperparameters\n",
    "5. Retrain the model using the optimised hyperparameters\n",
    "6. Re-evaluate the classifier using a number of metrics\n",
    "6. Save model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- scikit-learn classifiers, uncomment the one of interest----\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "import subprocess as sp\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `Classifier`: This parameter refers to the scikit-learn classification model to use, first uncomment the classifier of interest in the `Load Packages` section and then enter the function name into this parameter `e.g. Classifier = SVC`   \n",
    "* `class_dict`: A dictionary mapping the 'string' name of the classes to the integer values that represent our classes in the training data (e.g. `{'crop': 1., 'noncrop': 0.}`)\n",
    "* `ncpus`: Set this value to > 1 to parallize the model fitting eg. npus=8. \n",
    "* `metrics` : A single str or a list of strings to evaluate the predictions on the test set. See the scoring parameter page [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for a pre-defined list of options\n",
    "* `cv` : Determines the number of k-fold cross-validations to conduct during testing of the model.  A higher number will reduce the possibility of over-fitting, but will require more time to compute. 5 is a good default number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"results/training_data/test_training_data.txt\"\n",
    "\n",
    "Classifier = RandomForestClassifier\n",
    "\n",
    "class_dict = {'crop':1, 'noncrop':0}\n",
    "\n",
    "ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "metrics = 'f1' #good for binary classifications\n",
    "\n",
    "cv = 5\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:]\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing data\n",
    "\n",
    "Here will split the training data into four seperate datasets:\n",
    "* `train_features` : the feature layer data we will train the classifier on\n",
    "* `test_features` :  the feature layer data we will use to test the accuracy/precision of our classifier\n",
    "* `train_labels` : the dependent variables we will train the classifier on (in the default example the integers that represent the classes i.e. 1. and 0.)\n",
    "* `test_labels` : the dependent variables we will use to test the accuracy/precision of our classifier.\n",
    "\n",
    "Set the `test_size` to a fraction between 0 and 1, this will determine what fraction of the dataset will be set aside as the testing dataset. There is a trade-off here between having a larger test set that will help us better determine the quality of our classifier, and leaving enough data to train the classifier. A good deafult is to set 20 % of your dataset aside for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(model_input[:, model_col_indices],\n",
    "                                                                  model_input[:, 0],\n",
    "                                                                  test_size=test_size, \n",
    "                                                                  random_state=1)\n",
    "print(\"train_features shape:\", train_features.shape)\n",
    "print(\"test_features shape:\", test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Feature scaling\n",
    "\n",
    "Feature scaling (standardisaton or normalisation) of datasets is a common requirement for many machine learning estimators. For example, the objective function within the RBF kernel of Support Vector Machines assumes that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly.\n",
    "\n",
    "Below, we demonstrate the use of a common preprocessing function provided by [sklearn](https://scikit-learn.org/stable/modules/preprocessing.html), `StandardScaler`, which will standardise the values in an array to the array's mean and standard deviation via the formuala: `z = (x-u/s)`, where `u` is the mean of and `s` is the standard deviation.\n",
    "\n",
    "To centre all values around 0, set `with_mean = False` within the `StandardScalar()` method\n",
    "\n",
    "> **Note**: <ins>Always</ins> split your data into a training and test set **BEFORE** feature scaling (normalisation or standardisation) is applied. This is because the test set is intended to be a completly independent set of data. If feature scaling is applied before splitting, then the data will share a common mean and standard deviation (i.e. **information leakage** occurs between the training and test datasets).\n",
    "\n",
    "> **Note**: <ins>Do not</ins> apply standardisation to categorical data (dummy variables).\n",
    "\n",
    "> **Note**: If running this notebook using the default parameters, then there is no need to standardise the data since the default feature layers (surface reflectance & vegetation indices) are already normalised, and because Random Forest classifier aren't prone to these kinds of errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and variance for each feature\n",
    "sc = StandardScaler()\n",
    "\n",
    "#apply standard scalar params to the training set\n",
    "train_features = sc.fit_transform(train_features)\n",
    "\n",
    "#apply the same standard scalar params to the test set\n",
    "test_features = sc.fit_transform(test_features)\n",
    "\n",
    "# apply the same standard scalar params to all the feature data, \n",
    "# this is for our hyperparamter searching later on which relies on the full dataset\n",
    "model_input[:, model_col_indices] = sc.fit_transform(model_input[:, model_col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we reprint the data to compare it to the original `model_input` we printed above. The values have now been transformed to express their variance around the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names)\n",
    "print('')\n",
    "print(np.array_str(train_features, precision=2, suppress_small=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier\n",
    "\n",
    "The intial model will rely on the default parameters, during hyperparameter tuning later we will refine these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(random_state=1, n_jobs=ncpus, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier\n",
    "\n",
    "The following cells will help you examine the classifier and improve the results.  We can do this by:\n",
    "* Calculating the `cross-validation scores`, producing a `classification report` and a `confusion matrix`\n",
    "* Finding out which feature layers (bands in the input data) are most useful for classifying, and which are not,\n",
    "* Evaluating which model parameters (hyperparameters) optimize the model results \n",
    "* Plotting some of the decision trees from the random forest model to visualize how the algorithm is splitting the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics\n",
    "\n",
    "We can use the 20% sample of test data we partitioned earlier to test the accuracy of the trained model on this new, \"unseen\" data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on the test dataset\n",
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some accuracy metrics using k-fold cross-validation\n",
    "score = cross_val_score(model,\n",
    "                        model_input[:, model_col_indices],\n",
    "                        model_input[:, 0],\n",
    "                        cv=cv,\n",
    "                        scoring=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== Overall Accuracy ===\")\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(round(accuracy, 2))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== All \" +metrics+\" Scores ===\")\n",
    "print(np.array_str(score, precision=2, suppress_small=True))\n",
    "print('\\n')\n",
    "print(\"=== Mean \"+metrics+\" Score ===\")\n",
    "print(round(score.mean(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Feature Importance\n",
    "\n",
    "Extract classifier estimates of the relative importance of each band/variable for training the classifier. Useful for potentially selecting a subset of input bands/variables for model training/classification (i.e. optimising feature space). Results will be presented in descending order with the most important features listed first.  Importance is reported as a relative fraction between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the feature importance of the input features for predicting the class labels provided\n",
    "order = np.argsort(model.feature_importances_)\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.bar(x=np.array(column_names[1:])[order],\n",
    "        height=model.feature_importances_[order])\n",
    "plt.gca().set_ylabel('Importance', labelpad=10)\n",
    "plt.gca().set_xlabel('Variable', labelpad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters\n",
    "\n",
    "Hyperparameter searches are a required process in machine learning. Machine learning models require certain “hyperparameters”, model parameters that can be learned from the data. Finding these good values for these parameters is a “hyperparameter search” or an “hyperparameter optimization.”\n",
    "\n",
    "To optimize the parameters in our model, we will take a two-step approach. Firstly, we conduct a random search over many possible paramter values to narrow our search. Secondly, using the parameters returned from the random search, we use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to exhaustively search through the parameters and determine the combination that will result in the highest accuracy based upon the accuracy metric defined.\n",
    "\n",
    "* `param_grid`: Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored.\n",
    "\n",
    "> **Note**: the parameters in the `random_param_grid` object (and, later, the `param_grid` object) depend on the classifier being used. The default example is set up for a random forest classifier, to adjust ther paramaters to suit a different classifier, look up the important parameters under the relevant [sklearn documentation](https://scikit-learn.org/stable/supervised_learning.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random hyperparameter grid search\n",
    "\n",
    "> **Note**: Depending on the size of the dataset and the number of interations, `RandomizedSearchCV` can be slow to run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_param_grid = {\n",
    "     'bootstrap': [True, False],\n",
    "     'max_depth': [5, 10, 20, 30, 40, 50, 60, 70, 80, None],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4, 8],\n",
    "     'min_samples_split': [2, 5, 10, 20],\n",
    "     'n_estimators': [100, 200, 300, 400, 500, 600, 700]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the parameter searching\n",
    "random_grid_search = RandomizedSearchCV(model,\n",
    "                           random_param_grid,\n",
    "                           n_iter = 100,\n",
    "                           scoring=metrics,\n",
    "                           cv=cv,\n",
    "                           n_jobs=ncpus)\n",
    "\n",
    "random_grid_search.fit(model_input[:, model_col_indices], model_input[:, 0])\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(random_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search with Cross Validation\n",
    "\n",
    "Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with `GridSearchCV`, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [18, 19, 20, 22, 25],\n",
    "    'max_features': [\"auto\"],\n",
    "    'min_samples_leaf': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2,3,4,5],\n",
    "    'n_estimators': [750,800,850]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(model,\n",
    "                           param_grid, \n",
    "                           scoring=metrics,\n",
    "                           cv=cv,\n",
    "                           n_jobs=ncpus)\n",
    "\n",
    "grid_search.fit(model_input[:, model_col_indices], model_input[:, 0])\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model\n",
    "\n",
    "Using the best parameters from our hyperparmeter optmization search, we now rerun our model and re-test its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(**grid_search.best_params_, random_state=1, n_jobs=ncpus)\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score = cross_val_score(model,\n",
    "                        model_input[:, model_col_indices],\n",
    "                        model_input[:, 0],\n",
    "                        cv=cv,\n",
    "                        scoring=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== Overall Accuracy ===\")\n",
    "new_accuracy = accuracy_score(test_labels, predictions)\n",
    "print(round(new_accuracy, 2))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== All \" +metrics+\" Scores ===\")\n",
    "print(np.array_str(new_score, precision=2, suppress_small=True))\n",
    "print('\\n')\n",
    "print(\"=== Mean \"+metrics+\" Score ===\")\n",
    "print(round(new_score.mean(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Improvement in \"+metrics+\" score:\")\n",
    "print(new_score.mean() - score.mean())\n",
    "print('\\n')\n",
    "print(\"Improvement in overall accuracy:\")\n",
    "print(new_accuracy - accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "Running this cell will export the classifier as a binary`.joblib` file. This will allow for importing the model in the subsequent script, `4_Predict.ipynb` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, 'results/ml_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
