{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, fit, and evaluate classifier <img align=\"right\" src=\"../Supplementary_data/DE_Africa_Logo_Stacked_RGB_small.jpg\">\n",
    "\n",
    "\n",
    "TODO:\n",
    "- if/when datasets become large, consider implementing `dask_ml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "blahblahblah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "1. Split the training data into a training set and a test set\n",
    "2. Traing the model using a Random Forest Classifier\n",
    "3. Evaluate the classifier using a number of methods\n",
    "4. Optimise the model hyperparameters\n",
    "5. Retrain the model using the optimised hyperparameters\n",
    "6. Save model to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "import subprocess as sp\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split, cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters\n",
    "\n",
    "* `training_data`: Name and location of the training data `.txt` file output from runnning `1_Extract_training_data.ipynb`\n",
    "* `class_dict`: A dictionary mapping the 'string' name of the classes to the integer values that represent our classes in the training data (e.g. `{'crop': 1., 'noncrop': 0.}`)\n",
    "* `ncpus`: Set this value to > 1 to parallize the model fitting eg. npus=8. \n",
    "* `metrics` : A single str or a list of strings to evaluate the predictions on the test set. See the scoring parameter page [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for a pre-defined list of options\n",
    "* `cv` : Determines the number of k-fold cross-validations to conduct during testing of the model.  ahigher number here will reduce over-fitting, but require more time to compute 3-5 is a good default number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = \"results/training_data/test_training_data.txt\"\n",
    "\n",
    "class_dict = {'crop':1, 'noncrop':0}\n",
    "\n",
    "ncpus = int(float(sp.getoutput('env | grep CPU')[-3:]))\n",
    "\n",
    "metrics = 'f1'\n",
    "\n",
    "cv = 5\n",
    "\n",
    "print('ncpus = '+str(ncpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "model_input = np.loadtxt(training_data)\n",
    "\n",
    "# load the column_names\n",
    "with open(training_data, 'r') as file:\n",
    "    header = file.readline()\n",
    "    \n",
    "column_names = header.split()[1:]\n",
    "\n",
    "# Extract relevant indices from training data\n",
    "model_col_indices = [column_names.index(var_name) for var_name in column_names[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(model_input[:, model_col_indices],\n",
    "                                                                  model_input[:, 0],\n",
    "                                                                  test_size=0.2, \n",
    "                                                                  random_state=0)\n",
    "print(\"Train shape:\", train_features.shape)\n",
    "print(\"Test shape:\", test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "The intial model we use will rely on the default parameters, during hyperparameter tuning later we will alter these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state=1, n_job=ncpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier\n",
    "\n",
    "The following cells will help you examine the classifier and improve the results.  We can do this by:\n",
    "* Calculate the cross-validation scores, a classification report, and a confusion matrix\n",
    "* Finding out which features (bands in the input data) are most useful for classifying, and which are not,\n",
    "* Evaluating which model parameters will optimize the model \n",
    "* Plotting some of the decision trees from the random forest model to visualize how the algorithm is splitting the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics\n",
    "\n",
    "We can use the 20% sample of test data we partitioned earlier to test the accuracy of the trained model on this new, \"unseen\" data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(model,\n",
    "                        model_input[:, model_col_indices],\n",
    "                        model_input[:, 0],\n",
    "                        cv=cv,\n",
    "                        scoring=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== All \" +metrics+\" Scores ===\")\n",
    "print(score)\n",
    "print('\\n')\n",
    "print(\"=== Mean \"+metrics+\" Score ===\")\n",
    "print(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Feature Importance\n",
    "\n",
    "Extract classifier estimates of the relative importance of each band/variable for training the classifier. Useful for potentially selecting a subset of input bands/variables for model training/classification (i.e. optimising feature space). Results will be presented in descending order with the most important features listed first.  Importance is reported as a relative fraction between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the feature importance of the input features for predicting the class labels provided\n",
    "order = np.argsort(model.feature_importances_)\n",
    "\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.bar(x=np.array(column_names[1:])[order],\n",
    "        height=model.feature_importances_[order])\n",
    "plt.gca().set_ylabel('Importance', labelpad=10)\n",
    "plt.gca().set_xlabel('Variable', labelpad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize hyperparameters\n",
    "\n",
    "Hyperparameter searches are a required process in machine learning. Machine learning models require certain “hyperparameters”, model parameters that can be learned from the data. Finding these good values for these parameters is a “hyperparameter search” or an “hyperparameter optimization.”\n",
    "\n",
    "To optimize the parameters in our model, we will take a two-step approach. Firstly, we conduct a random search over many possible paramter values to narrow our search. Secondly, using the parameters returned from the random search, we use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to exhaustively search through the parameters and determine the combination that will result in the highest accuracy based upon the accuracy metric defined.\n",
    "\n",
    "* `param_grid`: Dictionary with parameters names (str) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_param_grid = {\n",
    "     'bootstrap': [True, False],\n",
    "     'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "     'max_features': ['auto', 'sqrt'],\n",
    "     'min_samples_leaf': [1, 2, 4],\n",
    "     'min_samples_split': [2, 5, 10],\n",
    "     'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the parameter searching\n",
    "random_grid_search = RandomizedSearchCV(model,\n",
    "                           random_param_grid,\n",
    "                           n_iter = 100,\n",
    "                           scoring=metrics,\n",
    "                           cv=cv,\n",
    "                           n_jobs=ncpus)\n",
    "\n",
    "random_grid_search.fit(model_input[:, model_col_indices], model_input[:, 0])\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(random_grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search with Cross Validation\n",
    "\n",
    "Random search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [70, 80, 90, 100],\n",
    "    'max_features': [\"sqrt\"],\n",
    "    'min_samples_leaf': [2, 3, 4, 5],\n",
    "    'min_samples_split': [2,3,4,5],\n",
    "    'n_estimators': [700,800,900]\n",
    "}\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(model,\n",
    "                           param_grid, \n",
    "                           scoring=metrics,\n",
    "                           cv=cv,\n",
    "                           n_jobs=ncpus)\n",
    "\n",
    "grid_search.fit(model_input[:, model_col_indices], model_input[:, 0])\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain model\n",
    "\n",
    "Using the best parameters from our hyperparmeter optmization search, we now rerun our model and re-test its accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(**grid_search.best_params_, random_state=1, n_job=ncpus)\n",
    "model.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score = cross_val_score(model,\n",
    "                        model_input[:, model_col_indices],\n",
    "                        model_input[:, 0],\n",
    "                        cv=cv,\n",
    "                        scoring=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Confusion Matrix ===\")\n",
    "print(confusion_matrix(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== Classification Report ===\")\n",
    "print(classification_report(test_labels, predictions))\n",
    "print('\\n')\n",
    "print(\"=== All \" +metrics+\" Scores ===\")\n",
    "print(new_score)\n",
    "print('\\n')\n",
    "print(\"=== Mean \"+metrics+\" Score ===\")\n",
    "print(new_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Improvement in \"+metric+\" score over default params:\")\n",
    "print(new_score.mean() - new_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export & plot tree diagrams\n",
    "\n",
    "Export .png plots of each decision tree in the random forest ensemble. Useful for inspecting the splits used by the classifier to classify the data.\n",
    "\n",
    "> This can be quite slow if the classifier/number of trees is quite large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 'results/tree_graphs/'\n",
    "for n, tree_in_forest in enumerate(model.estimators_):    \n",
    "\n",
    "    # Create graph and save to dot file\n",
    "    export_graphviz(tree_in_forest,\n",
    "                    out_file = loc+'tree.dot',\n",
    "                    feature_names = column_names[1:],\n",
    "                    class_names = list(class_dict.keys()),\n",
    "                    filled = True,\n",
    "                    rounded = True)\n",
    "\n",
    "    # Plot as figure\n",
    "    os.system('dot -Tpng ' + loc + 'tree.dot -o ' + loc + 'tree' + str(n + 1) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot any tree\n",
    "tree_number = 'tree1'\n",
    "\n",
    "img = plt.imread(loc + tree_number + '.png')\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.imshow(img, interpolation = \"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "Running this cell will export the classifier as a binary`.joblib` file. This will allow for importing the model in the subsequent script, `4_Predict.ipynb` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, 'results/ml_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
